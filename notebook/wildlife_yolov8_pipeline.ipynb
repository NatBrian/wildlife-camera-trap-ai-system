{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "## 1. Introduction & Objectives\n",
    "\n",
    "Camera traps are motion- or heat-triggered cameras placed in the wild that capture images whenever something moves through the scene. They produce huge numbers of images, many of which are empty; the valuable ones contain animals (sometimes multiple species at once). Object detection models add bounding boxes and class labels to each animal instance, which is more informative than image-level classification: you can count individuals, know where they are, and trigger actions only when animals appear.\n",
    "\n",
    "YOLOv8 is a modern one-stage detector that is fast, compact, and friendly to edge devices. In this notebook you'll build an end-to-end pipeline for a real camera-trap detection dataset (ENA24: Eastern North America) that includes bounding boxes and species labels. You'll learn **what** happens at every step and **why** it matters, from data cleaning through training, evaluation, and exporting models for lightweight edge deployments (e.g., a Raspberry Pi or Jetson that records clips only when wildlife is detected).\n",
    "\n",
    "**What we'll do**\n",
    "- Set up a Colab GPU environment with YOLOv8.\n",
    "- Download and explore a wildlife detection dataset with bounding boxes.\n",
    "- Clean annotations, convert COCO \u2192 YOLO format, and make a train/val split.\n",
    "- Train a small YOLOv8 model (transfer learning from COCO weights) suitable for Colab Free.\n",
    "- Evaluate with metrics (mAP, precision, recall) and visual inspection.\n",
    "- Run inference on sample images, including empty frames.\n",
    "- Export to ONNX/TorchScript for edge-device deployment.\n",
    "\n",
    "Throughout, you'll see why each step exists (e.g., cleaning boxes to avoid bad labels) and how it connects to an edge workflow that saves video clips only when animals enter the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 2. Setup and Dependencies\n",
    "\n",
    "- Colab GPUs (T4/P100) dramatically speed up training vs CPU.\n",
    "- YOLOv8 is chosen because it offers small, fast variants (\\`yolov8n\\`, \\`yolov8s\\`) that run well on edge hardware while keeping accuracy competitive.\n",
    "- We'll install only the essentials: YOLOv8 (\\`ultralytics\\`), Torch, COCO helpers, and plotting utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Check hardware and install dependencies\n",
    "import subprocess, sys\n",
    "\n",
    "gpu_info = subprocess.run(\"nvidia-smi -L\", shell=True, capture_output=True, text=True)\n",
    "if gpu_info.returncode == 0 and gpu_info.stdout.strip():\n",
    "    print(\"GPU detected:\\n\", gpu_info.stdout)\n",
    "else:\n",
    "    print(\"No GPU detected. Switch Colab to a GPU runtime for much faster training.\")\n",
    "\n",
    "packages = [\n",
    "    \"ultralytics==8.1.34\",  # YOLOv8\n",
    "    \"opencv-python\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"pyyaml\",\n",
    "    \"pycocotools\",\n",
    "    \"pandas\",\n",
    "    \"requests\"\n",
    "]\n",
    "print(\"Installing packages ... this takes ~1 minute on Colab\")\n",
    "subprocess.run(f\"pip install -q {' '.join(packages)}\", shell=True, check=True)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Imports, paths, and basic config\n",
    "import json, random, shutil, zipfile, tarfile, os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from IPython.display import Image, display\n",
    "from ultralytics import YOLO\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Base working folders (Colab uses /content by default)\n",
    "BASE_DIR = Path(\"/content/wildlife_yolov8\")\n",
    "DATA_ROOT = BASE_DIR / \"data\"\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "PROC_DIR = DATA_ROOT / \"processed\"\n",
    "YOLO_DIR = DATA_ROOT / \"wildlife_yolo\"\n",
    "for d in [BASE_DIR, DATA_ROOT, RAW_DIR, PROC_DIR, YOLO_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-explain"
   },
   "source": [
    "## 3. Downloading and Exploring the Dataset\n",
    "\n",
    "We use **ENA24** (Eastern North America) with COCO-style annotations. Two download options:\n",
    "- **Full zip**: grabs the entire dataset (~1\u20132 GB). Good if you want everything offline.\n",
    "- **Lightweight subset (metadata-driven)**: pulls the COCO JSON and only downloads a manageable number of annotated images (e.g., 500\u20131200) via HTTP. This is faster on Colab Free and mirrors the Gemini notebook's speed-focused approach.\n",
    "\n",
    "Camera-trap quirks to remember: many empty frames, night IR lighting, class imbalance, and occasional bad boxes. We'll clean and filter in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Download or locate the ENA24 detection dataset (COCO-style)\n",
    "import subprocess, requests\n",
    "\n",
    "# Choose download mode: \"full_zip\" (all data) or \"subset\" (faster, downloads only N annotated images)\n",
    "DOWNLOAD_MODE = \"subset\"  # options: \"subset\" or \"full_zip\"\n",
    "NUM_IMAGES_TO_DOWNLOAD = 800  # used only in subset mode; adjust for speed/coverage\n",
    "\n",
    "# Drive toggle if you already have the data there\n",
    "USE_DRIVE = False\n",
    "\n",
    "RAW_EXTRACT_DIR = RAW_DIR / \"ena24\"\n",
    "RAW_EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    RAW_EXTRACT_DIR = Path('/content/drive/MyDrive/ena24_detection')\n",
    "    RAW_EXTRACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Updated metadata and image URLs (with fallbacks across GCP/Azure/AWS)\n",
    "METADATA_URLS = [\n",
    "    \"https://lilawildlife.blob.core.windows.net/lila-wildlife/ena24/ena24.json\",\n",
    "    \"https://storage.googleapis.com/public-datasets-lila/ena24/ena24.json\",\n",
    "    \"http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/ena24/ena24.json\",\n",
    "]\n",
    "IMAGE_ZIP_URLS = [\n",
    "    \"https://storage.googleapis.com/public-datasets-lila/ena24/ena24.zip\",\n",
    "    \"https://lilawildlife.blob.core.windows.net/lila-wildlife/ena24/ena24.zip\",\n",
    "    \"http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/ena24/ena24.zip\",\n",
    "]\n",
    "# Base URL list for per-image downloads in subset mode\n",
    "IMAGE_BASE_URLS = [\n",
    "    \"https://lilawildlife.blob.core.windows.net/lila-wildlife/ena24/\",\n",
    "    \"https://storage.googleapis.com/public-datasets-lila/ena24/\",\n",
    "    \"http://us-west-2.opendata.source.coop.s3.amazonaws.com/agentmorris/lila-wildlife/ena24/\",\n",
    "]\n",
    "\n",
    "METADATA_PATH = RAW_EXTRACT_DIR / \"ena24_metadata.json\"\n",
    "ZIP_PATH = RAW_DIR / \"ena24_detection.zip\"\n",
    "\n",
    "def smart_extract(archive_path: Path, dest: Path):\n",
    "    \"\"\"Extract .zip or .tar.gz archives to dest.\"\"\"\n",
    "    if archive_path.suffix == '.zip' and zipfile.is_zipfile(archive_path):\n",
    "        with zipfile.ZipFile(archive_path, 'r') as zf:\n",
    "            zf.extractall(dest)\n",
    "    elif archive_path.suffixes[-2:] == ['.tar', '.gz']:\n",
    "        with tarfile.open(archive_path, 'r:gz') as tf:\n",
    "            tf.extractall(dest)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported archive type: {archive_path}\")\n",
    "\n",
    "def download_with_fallback(urls, dest):\n",
    "    for url in urls:\n",
    "        print(f\"Trying {url} ...\")\n",
    "        result = subprocess.run(f\"wget -O {dest} {url}\", shell=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Success: {url}\")\n",
    "            return url\n",
    "    raise RuntimeError(\"All download URLs failed. Please update the URL list or mount the data from Drive.\")\n",
    "\n",
    "if DOWNLOAD_MODE == \"full_zip\":\n",
    "    if not any(RAW_EXTRACT_DIR.iterdir()):\n",
    "        if not ZIP_PATH.exists():\n",
    "            print(\"Downloading ENA24 full zip with fallbacks...\")\n",
    "            download_with_fallback(IMAGE_ZIP_URLS, ZIP_PATH)\n",
    "        print(\"Extracting dataset ...\")\n",
    "        smart_extract(ZIP_PATH, RAW_EXTRACT_DIR)\n",
    "    else:\n",
    "        print(\"Dataset already present, skipping download/extract.\")\n",
    "\n",
    "    image_dirs = sorted([p for p in RAW_EXTRACT_DIR.rglob('images') if p.is_dir()])\n",
    "    annotation_files = sorted([p for p in RAW_EXTRACT_DIR.rglob('*.json')])\n",
    "    if len(image_dirs) == 0:\n",
    "        raise FileNotFoundError(\"Could not find an 'images' directory. Inspect RAW_EXTRACT_DIR and adjust paths.\")\n",
    "    if len(annotation_files) == 0:\n",
    "        raise FileNotFoundError(\"Could not find any JSON annotation file. Please verify the dataset download.\")\n",
    "    IMAGES_ROOT = image_dirs[0]\n",
    "    ANNOTATION_PATH = annotation_files[0]\n",
    "\n",
    "else:  # subset mode: download metadata + only N annotated images\n",
    "    if not METADATA_PATH.exists():\n",
    "        print(\"Downloading metadata JSON (fallbacks)...\")\n",
    "        success = False\n",
    "        for url in METADATA_URLS:\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=30)\n",
    "                resp.raise_for_status()\n",
    "                METADATA_PATH.write_bytes(resp.content)\n",
    "                print(\"Metadata saved to\", METADATA_PATH)\n",
    "                success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed metadata URL {url}: {e}\")\n",
    "        if not success:\n",
    "            raise RuntimeError(\"All metadata URLs failed. Update METADATA_URLS or mount the file from Drive.\")\n",
    "    else:\n",
    "        print(\"Metadata already present.\")\n",
    "\n",
    "    with open(METADATA_PATH, 'r') as f:\n",
    "        meta_coco = json.load(f)\n",
    "\n",
    "    annotated_image_ids = set(ann['image_id'] for ann in meta_coco['annotations'])\n",
    "    valid_images = [img for img in meta_coco['images'] if img['id'] in annotated_image_ids]\n",
    "    random.shuffle(valid_images)\n",
    "    subset = valid_images[:min(NUM_IMAGES_TO_DOWNLOAD, len(valid_images))]\n",
    "    print(f\"Downloading {len(subset)} annotated images (subset mode)...\")\n",
    "\n",
    "    IMAGES_ROOT = RAW_EXTRACT_DIR / \"images_subset\"\n",
    "    IMAGES_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for img in subset:\n",
    "        rel_path = Path(img['file_name'])\n",
    "        dest_path = IMAGES_ROOT / rel_path\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if dest_path.exists():\n",
    "            continue\n",
    "        downloaded = False\n",
    "        for base in IMAGE_BASE_URLS:\n",
    "            url = f\"{base}{img['file_name']}\"\n",
    "            try:\n",
    "                r = requests.get(url, stream=True, timeout=30)\n",
    "                if r.status_code == 200:\n",
    "                    with open(dest_path, 'wb') as f_out:\n",
    "                        f_out.write(r.content)\n",
    "                    downloaded = True\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Failed {url}: {e}\")\n",
    "        if not downloaded:\n",
    "            print(f\"Failed to download image {img['file_name']} from all bases.\")\n",
    "\n",
    "    ANNOTATION_PATH = METADATA_PATH\n",
    "\n",
    "print(f\"Using images from: {IMAGES_ROOT}\")\n",
    "print(f\"Using annotations: {ANNOTATION_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore"
   },
   "outputs": [],
   "source": [
    "# Explore the raw data: inspect COCO structure and visualize a few examples\n",
    "\n",
    "with open(ANNOTATION_PATH, 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "print(f\"Images listed in COCO: {len(coco['images'])}\")\n",
    "print(f\"Annotations: {len(coco['annotations'])}\")\n",
    "print(f\"Categories: {len(coco['categories'])}\")\n",
    "\n",
    "category_id_to_name = {c['id']: c['name'] for c in coco['categories']}\n",
    "print(\"Sample categories:\", list(category_id_to_name.values())[:10])\n",
    "\n",
    "def resolve_image_path(file_name: str) -> Path:\n",
    "    \"\"\"Best-effort resolver for file paths regardless of stored prefix.\"\"\"\n",
    "    direct = IMAGES_ROOT / file_name\n",
    "    if direct.exists():\n",
    "        return direct\n",
    "    base = IMAGES_ROOT / Path(file_name).name\n",
    "    if base.exists():\n",
    "        return base\n",
    "    return direct  # fallback (even if missing)\n",
    "\n",
    "def draw_coco_example(image_meta, annotations):\n",
    "    \"\"\"Load one image and draw its bounding boxes with labels.\"\"\"\n",
    "    img_path = resolve_image_path(image_meta['file_name'])\n",
    "    if not img_path.exists():\n",
    "        print(f\"Missing image: {img_path}\")\n",
    "        return\n",
    "    img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    for ann in annotations:\n",
    "        x, y, w, h = ann['bbox']\n",
    "        rect = plt.Rectangle((x, y), w, h, fill=False, color='lime', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        label = category_id_to_name.get(ann['category_id'], 'unknown')\n",
    "        ax.text(x, y - 2, label, color='yellow', fontsize=10, bbox=dict(facecolor='black', alpha=0.5, pad=1))\n",
    "    plt.show()\n",
    "\n",
    "# Pick a few images that actually have annotations (skip empty frames)\n",
    "ann_by_image = defaultdict(list)\n",
    "for ann in coco['annotations']:\n",
    "    ann_by_image[ann['image_id']].append(ann)\n",
    "\n",
    "# Only keep images with at least one annotation\n",
    "image_with_boxes = [img for img in coco['images'] if len(ann_by_image[img['id']]) > 0]\n",
    "for sample in random.sample(image_with_boxes, min(3, len(image_with_boxes))):\n",
    "    draw_coco_example(sample, ann_by_image[sample['id']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clean-transform"
   },
   "source": [
    "## 4. Cleaning and Transforming the Data\n",
    "\n",
    "Why clean?\n",
    "- Broken images or impossible bounding boxes (negative sizes) harm training.\n",
    "- Camera traps have many empty frames and long-tail classes; we'll keep the top-N most frequent species and optionally cap the number of images to stay within Colab Free limits.\n",
    "- YOLO needs one text file per image (\\`class_id x_center y_center width height\\`, normalized 0\u20131). We'll convert from COCO's pixel-space format.\n",
    "\n",
    "Controls in the next cell:\n",
    "- \\`TOP_K_CLASSES\\`: keep the most common classes.\n",
    "- \\`MAX_IMAGES\\`: limit total images for a quick run.\n",
    "- \\`MIN_BOX_AREA\\`: drop tiny or zero-area boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "convert"
   },
   "outputs": [],
   "source": [
    "# Clean annotations and convert COCO \u2192 YOLO format\n",
    "TOP_K_CLASSES = 6         # keep the most common classes\n",
    "MAX_IMAGES = 1200         # None for full data, but 1.2k keeps Colab runs reasonable\n",
    "MIN_BOX_AREA = 16         # drop boxes that are too small to be reliable\n",
    "\n",
    "image_id_to_meta = {img['id']: img for img in coco['images']}\n",
    "ann_by_image = defaultdict(list)\n",
    "for ann in coco['annotations']:\n",
    "    ann_by_image[ann['image_id']].append(ann)\n",
    "\n",
    "# Remove invalid boxes and collect basic stats\n",
    "valid_annotations = []\n",
    "for ann in coco['annotations']:\n",
    "    img_meta = image_id_to_meta.get(ann['image_id'])\n",
    "    if img_meta is None:\n",
    "        continue\n",
    "    x, y, w, h = ann['bbox']\n",
    "    if w <= 0 or h <= 0 or w * h < MIN_BOX_AREA:\n",
    "        continue\n",
    "    valid_annotations.append(ann)\n",
    "\n",
    "present_class_counts = Counter([\n",
    "    a['category_id']\n",
    "    for a in valid_annotations\n",
    "    if resolve_image_path(image_id_to_meta[a['image_id']]['file_name']).exists()\n",
    "])\n",
    "keep_categories = [cid for cid, _ in present_class_counts.most_common(TOP_K_CLASSES)]\n",
    "print(\"Top classes (after presence check):\")\n",
    "for cid in keep_categories:\n",
    "    print(f\"  {cid}: {category_id_to_name[cid]} (count={present_class_counts[cid]})\")\n",
    "\n",
    "if not keep_categories:\n",
    "    raise ValueError('No categories found after filtering. Adjust TOP_K_CLASSES, MAX_IMAGES, or ensure images downloaded.')\n",
    "filtered_annotations = [\n",
    "    a for a in valid_annotations\n",
    "    if a['category_id'] in keep_categories\n",
    "    and resolve_image_path(image_id_to_meta[a['image_id']]['file_name']).exists()\n",
    "]\n",
    "images_with_kept_classes = sorted({a['image_id'] for a in filtered_annotations})\n",
    "\n",
    "# Keep only those whose image files actually exist (important in subset mode)\n",
    "existing_images = []\n",
    "for img_id in images_with_kept_classes:\n",
    "    meta = image_id_to_meta[img_id]\n",
    "    img_path = resolve_image_path(meta['file_name'])\n",
    "    if img_path.exists():\n",
    "        existing_images.append(img_id)\n",
    "\n",
    "if MAX_IMAGES is not None:\n",
    "    random.shuffle(existing_images)\n",
    "    existing_images = existing_images[:MAX_IMAGES]\n",
    "print(f\"Images kept after filtering & existence check: {len(existing_images)}\")\n",
    "\n",
    "# Remap category IDs to 0..N-1 in a stable order\n",
    "sorted_cats = sorted(keep_categories)\n",
    "cat_id_to_yolo = {cid: idx for idx, cid in enumerate(sorted_cats)}\n",
    "class_names = [category_id_to_name[cid] for cid in sorted_cats]\n",
    "\n",
    "labels_dir = PROC_DIR / \"labels\"\n",
    "images_dir = PROC_DIR / \"images\"\n",
    "labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "converted = []  # list of {image_path, label_path}\n",
    "skipped_missing = 0\n",
    "\n",
    "for img_id in existing_images:\n",
    "    meta = image_id_to_meta[img_id]\n",
    "    src_img_path = resolve_image_path(meta['file_name'])\n",
    "    if not src_img_path.exists():\n",
    "        skipped_missing += 1\n",
    "        continue\n",
    "    stem = Path(meta['file_name']).stem\n",
    "    ext = Path(meta['file_name']).suffix or '.jpg'\n",
    "    dst_img_path = images_dir / f\"{stem}{ext}\"\n",
    "    dst_label_path = labels_dir / f\"{stem}.txt\"\n",
    "\n",
    "    # Copy image into the processed folder (small subset keeps storage reasonable)\n",
    "    if not dst_img_path.exists():\n",
    "        shutil.copy2(src_img_path, dst_img_path)\n",
    "\n",
    "    lines = []\n",
    "    for ann in ann_by_image[img_id]:\n",
    "        if ann['category_id'] not in keep_categories:\n",
    "            continue\n",
    "        x, y, w, h = ann['bbox']\n",
    "        x_c = (x + w / 2) / meta['width']\n",
    "        y_c = (y + h / 2) / meta['height']\n",
    "        w_n = w / meta['width']\n",
    "        h_n = h / meta['height']\n",
    "        # Clamp to [0,1] to avoid edge numeric issues\n",
    "        x_c = min(max(x_c, 0.0), 1.0)\n",
    "        y_c = min(max(y_c, 0.0), 1.0)\n",
    "        w_n = min(max(w_n, 0.0), 1.0)\n",
    "        h_n = min(max(h_n, 0.0), 1.0)\n",
    "        cls = cat_id_to_yolo[ann['category_id']]\n",
    "        lines.append(f\"{cls} {x_c:.6f} {y_c:.6f} {w_n:.6f} {h_n:.6f}\")\n",
    "\n",
    "    if lines:\n",
    "        with open(dst_label_path, 'w') as f:\n",
    "            f.write(\"\\n\".join(lines))\n",
    "        converted.append({\"image_path\": dst_img_path, \"label_path\": dst_label_path})\n",
    "\n",
    "if not converted:\n",
    "    raise ValueError('No images were converted. Check download mode, paths, or filtering settings.')\n",
    "print(f\"Converted {len(converted)} images. Missing images skipped: {skipped_missing}\")\n",
    "print(f\"Class names (YOLO order): {class_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare"
   },
   "source": [
    "## 5. Preparing YOLOv8 Dataset Structure\n",
    "\n",
    "YOLO expects:\n",
    "```\n",
    "data/wildlife_yolo/\n",
    "  images/train, images/val\n",
    "  labels/train, labels/val  (same filenames, .txt)\n",
    "wildlife.yaml  (paths + class names)\n",
    "```\n",
    "- We'll split train/val (80/20), copy files into the YOLO layout, and write a YAML config.\n",
    "- The order of \\`names\\` in the YAML **must** match the numeric class IDs we wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare-code"
   },
   "outputs": [],
   "source": [
    "# Create YOLO directory tree and train/val split\n",
    "train_ratio = 0.8\n",
    "random.shuffle(converted)\n",
    "split_idx = int(len(converted) * train_ratio)\n",
    "train_items = converted[:split_idx]\n",
    "val_items = converted[split_idx:]\n",
    "if len(val_items) == 0 and len(train_items) > 1:\n",
    "    val_items.append(train_items.pop())  # ensure we have a val set for metrics\n",
    "elif len(val_items) == 0:\n",
    "    raise ValueError('Not enough images to create a validation split. Reduce filtering or MAX_IMAGES.')\n",
    "\n",
    "for subset, items in [(\"train\", train_items), (\"val\", val_items)]:\n",
    "    (YOLO_DIR / \"images\" / subset).mkdir(parents=True, exist_ok=True)\n",
    "    (YOLO_DIR / \"labels\" / subset).mkdir(parents=True, exist_ok=True)\n",
    "    for entry in items:\n",
    "        dst_img = YOLO_DIR / \"images\" / subset / entry['image_path'].name\n",
    "        dst_lbl = YOLO_DIR / \"labels\" / subset / entry['label_path'].name\n",
    "        if not dst_img.exists():\n",
    "            shutil.copy2(entry['image_path'], dst_img)\n",
    "        if not dst_lbl.exists():\n",
    "            shutil.copy2(entry['label_path'], dst_lbl)\n",
    "\n",
    "yaml_path = YOLO_DIR / \"wildlife.yaml\"\n",
    "yaml_dict = {\n",
    "    \"path\": str(YOLO_DIR),\n",
    "    \"train\": \"images/train\",\n",
    "    \"val\": \"images/val\",\n",
    "    \"names\": class_names\n",
    "}\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.safe_dump(yaml_dict, f)\n",
    "\n",
    "print(f\"Train images: {len(train_items)}, Val images: {len(val_items)}\")\n",
    "print(f\"Wrote dataset YAML to: {yaml_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-explain"
   },
   "source": [
    "## 6. Training YOLOv8\n",
    "\n",
    "Key ideas:\n",
    "- **Transfer learning**: start from a COCO-pretrained checkpoint (\\`yolov8n.pt\\`) so we learn faster with fewer epochs.\n",
    "- **Hyperparameters**: \n",
    "  - \\`epochs\\`: passes over the data (20\u201340 is fine for this subset).\n",
    "  - \\`batch\\`: images per step (increase until GPU memory complaints).\n",
    "  - \\`imgsz\\`: resize dimension; 640 balances detail vs speed.\n",
    "  - \\`patience\\`: early stopping if metrics stop improving.\n",
    "- **Model size trade-off**: \\`yolov8n\\` (nano) is fastest/lightest; \\`yolov8s\\` is a bit slower but more accurate. We'll use nano for Colab Free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Train a YOLOv8 nano model\n",
    "batch_size = 16 if torch.cuda.is_available() else 4\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "train_results = model.train(\n",
    "    data=str(yaml_path),\n",
    "    epochs=30,\n",
    "    imgsz=640,\n",
    "    batch=batch_size,\n",
    "    patience=5,\n",
    "    name='ena24_yolov8n',\n",
    "    project=str(BASE_DIR / 'runs'),\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "print(\"Training complete. Best weights saved under:\", Path(model.trainer.save_dir) / 'weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-curves"
   },
   "outputs": [],
   "source": [
    "# Visualize training curves (loss, mAP, precision/recall) similar to the Gemini notebook summary\n",
    "results_png = Path(model.trainer.save_dir) / \"results.png\"\n",
    "if results_png.exists():\n",
    "    display(Image(filename=str(results_png)))\n",
    "else:\n",
    "    print(\"results.png not found; verify training run path:\", model.trainer.save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate-explain"
   },
   "source": [
    "## 7. Evaluating the Model\n",
    "\n",
    "- **mAP (mean Average Precision)**: area under the precision\u2013recall curve across IoU thresholds and classes. Higher is better.\n",
    "- **Precision**: of the predicted boxes, how many are correct (controls false positives).\n",
    "- **Recall**: of the ground-truth boxes, how many did we find (controls false negatives).\n",
    "- Visual inspection matters: numbers can look fine even if the model systematically confuses similar species. We'll inspect a few val images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Quantitative validation\n",
    "best_weights = Path(model.trainer.save_dir) / 'weights' / 'best.pt'\n",
    "if not best_weights.exists():\n",
    "    raise FileNotFoundError('best.pt not found. Check training run or path.')\n",
    "val_model = YOLO(best_weights)\n",
    "val_results = val_model.val()\n",
    "print(val_results)\n",
    "\n",
    "# Visualize a handful of predictions on the val set\n",
    "sample_paths = [entry['image_path'] for entry in random.sample(val_items, min(3, len(val_items)))]\n",
    "if sample_paths:\n",
    "    preds = val_model.predict(sample_paths, imgsz=640, conf=0.25)\n",
    "    for res in preds:\n",
    "        plotted = res.plot()  # BGR image with boxes drawn\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cv2.cvtColor(plotted, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "else:\n",
    "    print('No validation images available for visualization.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference-explain"
   },
   "source": [
    "## 8. Running Inference on Sample Images\n",
    "\n",
    "We'll build a small helper that takes an image path, runs the model, and plots the detections. Try both a crowded scene and an empty frame to see false positives.\n",
    "\n",
    "On an edge device, replace the file path with frames from a webcam or video stream; the rest (model call + thresholding) is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference"
   },
   "outputs": [],
   "source": [
    "def run_inference(image_path, model, conf=0.25):\n",
    "    \"\"\"Run YOLO on one image and display results.\"\"\"\n",
    "    res = model.predict(image_path, imgsz=640, conf=conf)[0]\n",
    "    plotted = res.plot()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cv2.cvtColor(plotted, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return res\n",
    "\n",
    "# Choose two examples (one possibly empty)\n",
    "if len(val_items) >= 2:\n",
    "    crowded = val_items[0]['image_path']\n",
    "    maybe_empty = val_items[-1]['image_path']\n",
    "    print(\"Crowded example:\")\n",
    "    _ = run_inference(crowded, val_model, conf=0.25)\n",
    "    print(\"Potential empty/low-activity example:\")\n",
    "    _ = run_inference(maybe_empty, val_model, conf=0.10)\n",
    "else:\n",
    "    print(\"Not enough validation images to demo inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "export-explain"
   },
   "source": [
    "## 9. Exporting the Model for Edge Deployment\n",
    "\n",
    "- Find \\`best.pt\\` in the run folder.\n",
    "- Export to **ONNX** (works in many runtimes, C++/Rust) and **TorchScript** (for PyTorch mobile/edge).\n",
    "- Smaller models (\\`yolov8n\\`) or INT8 quantization are preferred on Raspberry Pi/Jetson. Use lower \\`imgsz\\` (e.g., 416) to trade detail for speed if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "export"
   },
   "outputs": [],
   "source": [
    "# Export best weights to ONNX and TorchScript\n",
    "export_model = YOLO(best_weights)\n",
    "onnx_path = export_model.export(format='onnx', imgsz=640, opset=12, dynamic=True)\n",
    "ts_path = export_model.export(format='torchscript', imgsz=640)\n",
    "\n",
    "print(\"Exported:\")\n",
    "print(\"  ONNX:\", onnx_path)\n",
    "print(\"  TorchScript:\", ts_path)\n",
    "\n",
    "print(\"Files in weights folder:\")\n",
    "for p in (Path(best_weights).parent).glob('*'):\n",
    "    print(\" -\", p.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## 10. Next Steps and Extensions\n",
    "\n",
    "- Train on a larger mix (e.g., Caltech Camera Traps, Snapshot Serengeti, WCS) or your own camera data; merge datasets by harmonizing class names.\n",
    "- Group rare species into broader buckets (e.g., \"bird\", \"deer\", \"predator\") to reduce class imbalance.\n",
    "- Add **temporal cues**: run detection over short clips or consecutive frames to suppress false positives on empty frames.\n",
    "- Experiment with **augmentation** (mixup, mosaic, random brightness) to improve low-light/night robustness.\n",
    "- Deploy to edge: quantize the ONNX model (e.g., with ONNX Runtime or TensorRT), run inference in a loop, and trigger video recording or SMS alerts only when confidence crosses a threshold.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
